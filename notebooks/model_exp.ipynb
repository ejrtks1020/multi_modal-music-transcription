{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mfcc_path = \"/opt/ml/ytdataset/audios_mfcc/train/100_Paul Rice “Waltz for Ella” FREE SHEET MUSIC P Barton FEURICH piano.mp4.pickle\"\n",
    "mfcc = np.load(mfcc_path, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_arr = np.asarray([mfcc[i] for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 13, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "total_time = []\n",
    "for i in range(100):\n",
    "    x = [np.random.rand(100, 900) for _ in range(5)]\n",
    "    st = time.time()\n",
    "    torch.Tensor(np.asarray(x)).float().cuda()\n",
    "    ed = time.time()\n",
    "    total_time.append((ed -st) * 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.080057144165039,\n",
       " 11.507272720336914,\n",
       " 5.411624908447266,\n",
       " 3.293752670288086,\n",
       " 3.209352493286133,\n",
       " 3.1833648681640625,\n",
       " 2.3069381713867188,\n",
       " 1.260995864868164,\n",
       " 1.3887882232666016,\n",
       " 1.8720626831054688,\n",
       " 3.175020217895508,\n",
       " 3.1714439392089844,\n",
       " 3.262042999267578,\n",
       " 2.744436264038086,\n",
       " 3.1833648681640625,\n",
       " 3.217935562133789,\n",
       " 3.1731128692626953,\n",
       " 3.1888484954833984,\n",
       " 3.1807422637939453,\n",
       " 3.1893253326416016,\n",
       " 1.4352798461914062,\n",
       " 1.3113021850585938,\n",
       " 1.268148422241211,\n",
       " 1.257181167602539,\n",
       " 1.2521743774414062,\n",
       " 1.2812614440917969,\n",
       " 1.2586116790771484,\n",
       " 1.253366470336914,\n",
       " 3.1280517578125,\n",
       " 3.1528472900390625,\n",
       " 3.1349658966064453,\n",
       " 3.219127655029297,\n",
       " 3.1366348266601562,\n",
       " 2.765655517578125,\n",
       " 3.150463104248047,\n",
       " 3.1380653381347656,\n",
       " 3.1452178955078125,\n",
       " 3.117084503173828,\n",
       " 3.1538009643554688,\n",
       " 3.1280517578125,\n",
       " 3.118276596069336,\n",
       " 3.147602081298828,\n",
       " 1.0852813720703125,\n",
       " 1.1088848114013672,\n",
       " 1.108407974243164,\n",
       " 1.1470317840576172,\n",
       " 1.1477470397949219,\n",
       " 2.3508071899414062,\n",
       " 1.1060237884521484,\n",
       " 2.1936893463134766,\n",
       " 1.1289119720458984,\n",
       " 1.1126995086669922,\n",
       " 1.1234283447265625,\n",
       " 1.1055469512939453,\n",
       " 1.0986328125,\n",
       " 1.110076904296875,\n",
       " 1.1210441589355469,\n",
       " 1.1053085327148438,\n",
       " 1.0993480682373047,\n",
       " 1.1086463928222656,\n",
       " 1.1336803436279297,\n",
       " 1.1208057403564453,\n",
       " 1.1050701141357422,\n",
       " 1.1072158813476562,\n",
       " 1.1157989501953125,\n",
       " 1.1289119720458984,\n",
       " 1.1150836944580078,\n",
       " 1.1081695556640625,\n",
       " 1.1107921600341797,\n",
       " 1.6493797302246094,\n",
       " 1.0802745819091797,\n",
       " 1.1377334594726562,\n",
       " 1.1186599731445312,\n",
       " 3.2558441162109375,\n",
       " 1.4770030975341797,\n",
       " 1.112222671508789,\n",
       " 1.1208057403564453,\n",
       " 1.1081695556640625,\n",
       " 1.1286735534667969,\n",
       " 3.1244754791259766,\n",
       " 3.1528472900390625,\n",
       " 2.2649765014648438,\n",
       " 3.1270980834960938,\n",
       " 3.1442642211914062,\n",
       " 6.56437873840332,\n",
       " 3.2007694244384766,\n",
       " 3.1757354736328125,\n",
       " 3.1626224517822266,\n",
       " 3.1502246856689453,\n",
       " 3.158092498779297,\n",
       " 3.1723976135253906,\n",
       " 3.175497055053711,\n",
       " 3.733396530151367,\n",
       " 3.1709671020507812,\n",
       " 3.1468868255615234,\n",
       " 3.1375885009765625,\n",
       " 3.144502639770508,\n",
       " 3.124713897705078,\n",
       " 3.1342506408691406,\n",
       " 3.1371116638183594]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0802745819091797, 2.7550458908081055)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(np.asarray(total_time)), np.median(np.asarray(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(5, 32, 2, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 2, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((11, 1)),\n",
    "    # nn.AdaptiveAvgPool2d((1,1))\n",
    "    # nn.Conv2d(32, 64, 3, 1, 1),\n",
    "    # nn.ReLU(),\n",
    "    # nn.MaxPool2d(2, 2)\n",
    "    nn.Flatten(start_dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64]) torch.Size([1, 5, 13, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(mfcc_arr).float().unsqueeze(0)\n",
    "print(net(x).shape, x.shape)\n",
    "# torch.nn.init.xavier_uniform_(\n",
    "#     self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(5, 32, 2, 1),\n",
    "            nn.ReLU(),            \n",
    "            nn.Conv2d(32, 64, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((11, 1)),\n",
    "            nn.Flatten(start_dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        return x\n",
    "\n",
    "# 간단한 RNN 모델 정의\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = CNNModel()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1764, hidden_size=256, num_layers=3, output_size=64):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 hidden state와 cell state 설정\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # LSTM 모델 적용\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        print(out.shape)\n",
    "\n",
    "        # 마지막 타임스텝의 hidden state를 추출하여 특징 벡터로 변환\n",
    "        features = self.fc(out[:, -1, :])\n",
    "\n",
    "        return features\n",
    "    \n",
    "\n",
    "class RESTM(nn.Module):\n",
    "    def __init__(self, pretrained=False, min_key=5, max_key=80, lstm_output_size=256, resnet_output_size=256):\n",
    "        super(RESTM, self).__init__()\n",
    "        self.num_classes=(max_key-min_key)+1\n",
    "\n",
    "        self.lstm = LSTMModel(output_size=lstm_output_size)\n",
    "        self.resnet = ResNet(BasicBlock, layers=[3, 3, 3, 3, 2], top_channel_nums=2048, reduced_channel_nums=256, num_classes=resnet_output_size)\n",
    "        if pretrained:\n",
    "            model_path = '/opt/ml/Audeo/Audeo_github/Video2Roll_models/Video2RollNet.pth' # change to your path\n",
    "            checkpoint = torch.load(model_path, map_location=\"cuda:0\")\n",
    "            checkpoint['fc.weight'] = self.resnet.fc.weight\n",
    "            checkpoint['fc.bias'] = self.resnet.fc.bias\n",
    "            self.resnet.cuda()\n",
    "            self.resnet.load_state_dict(checkpoint)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_output_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, self.num_classes))\n",
    "    \n",
    "    def forward(self, x_frames, x_audio):\n",
    "        x_frames = self.resnet(x_frames)\n",
    "        x_audio = self.lstm(x_audio)\n",
    "        # x_frames_audio = torch.concatenate((x_frames, x_audio), dim=1)]\n",
    "        x = x_frames + x_audio\n",
    "        y = self.fc(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if (1,2):\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (862584758.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[44], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    *[(1, 3), (1, 3)]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "*[(1, 3), (1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 5, 100, 900)\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(5, 32, 3, 1, 1),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(32, 64, 3, 1, 1),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(64, 64, 3, 1, 1),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(64, 128, 3, 1, 1),\n",
    "    nn.MaxPool2d((1, 2), (1, 2)),\n",
    "    nn.Conv2d(128, 128, 3, 1, 1),\n",
    "    nn.MaxPool2d((1, 2), (1, 2)),\n",
    "    nn.Conv2d(128, 256, 3, 1, 1),\n",
    "    nn.MaxPool2d((1, 2), (1, 2)),\n",
    "    nn.Conv2d(256, 512, 3, 1, 1),\n",
    "    nn.MaxPool2d((1, 3), (1, 1)),\n",
    "    nn.Conv2d(512, 512, 3, 1, 1),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.AdaptiveAvgPool2d((1,1))\n",
    "    )\n",
    "fc = nn.Linear(512, 64)\n",
    "\n",
    "x = net(x)\n",
    "x = fc(x.squeeze())\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 100, 900])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 64])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(1764, 256, 2, batch_first=True)\n",
    "lstm2 = nn.LSTM(256, 64, 2, batch_first=True)\n",
    "\n",
    "out = lstm(torch.randn(2, 5, 1764))[0]\n",
    "out = lstm2(out)[0]\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 256])\n",
      "torch.Size([2, 76])\n"
     ]
    }
   ],
   "source": [
    "model = RESTM()\n",
    "x_frames = torch.randn(2, 5, 100, 900)\n",
    "x_audio = torch.randn(2, 5, 1764)\n",
    "print(model(x_frames, x_audio).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m model \u001b[39m=\u001b[39m TransformerModel(input_size, hidden_size, num_heads, num_layers)\n\u001b[1;32m     28\u001b[0m input_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m, input_size)  \u001b[39m# Replace with your own input data\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m output \u001b[39m=\u001b[39m model(input_data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m     13\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# Reshape to (seq_len, batch_size, hidden_size)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m     15\u001b[0m x \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# Take the output of the last transformer layer\u001b[39;00m\n\u001b[1;32m     16\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# Reshape back to (batch_size, seq_len, hidden_size)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(d_model=hidden_size, nhead=num_heads, num_encoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # Reshape to (seq_len, batch_size, hidden_size)\n",
    "        outputs = self.transformer(x)\n",
    "        x = outputs[0]  # Take the output of the last transformer layer\n",
    "        x = x.permute(1, 0, 2)  # Reshape back to (batch_size, seq_len, hidden_size)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Usage example\n",
    "input_size = 1764  # Input size of the audio data\n",
    "hidden_size = 256  # Hidden size of the Transformer model\n",
    "num_heads = 4  # Number of attention heads\n",
    "num_layers = 4  # Number of transformer layers\n",
    "num_classes = 10  # Number of output classes\n",
    "\n",
    "model = TransformerModel(input_size, hidden_size, num_heads, num_layers)\n",
    "input_data = torch.randn(1, 5, input_size)  # Replace with your own input data\n",
    "output = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56448"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 * 1764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 1764])\n",
      "torch.Size([64, 32, 882])\n",
      "torch.Size([64, 32, 441])\n",
      "torch.Size([64, 14112])\n",
      "torch.Size([64, 76])\n"
     ]
    }
   ],
   "source": [
    "class AudioCNNModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(AudioCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(32 * 1764 // 4, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(x.shape)\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(x.shape)\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Usage example\n",
    "input_channels = 5  # Number of input channels (e.g., mono audio)\n",
    "num_classes = 76  # Number of output classes\n",
    "\n",
    "model = AudioCNNModel(input_channels, num_classes)\n",
    "input_data = torch.randn(64, 5, 1764)  # Replace with your own input data\n",
    "output = model(input_data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
